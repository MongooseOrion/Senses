# 微软开发者大会：Build 2023 会议记录

  * 时间：中国标准时间 2023 年 5 月 24 日 - 2023 年 5 月 25 日
  * 关键词：AI、Copilot、GPT、ARM

## 主要议题

  * Copilot 改变所有人的生活方式
  * Windows ARM 取得新进展
  * GPT 的基本原理



## 属于 Copilot AI 的新纪元来临

### 宣布更新内容

  1. Microsoft 将 Copilot 服务扩展到多个应用和服务中，包括 Microsoft 365、GitHub Copilot、Windows Copilot，同时Microsoft 希望将 Copilot 作为一项基本服务，扩展为人人的 Copilot，并可供开发用户使用；
  2. Windows Copilot 发布，它将固定在任务栏，支持多模态的输入，为用户提供操作建议，并生成本地工作流和日常程序；
  3. 利用 Microsoft 365 Copilot 你可以通过简单的问询，在 Word 中生成书面文本，这一切都得益于 Bing 的联网搜索能力；
  4. Bing 现在加入 ChatGPT plus 插件，这意味着 ChatGPT 也将具备联网搜索能力，同时也将来到免费版；
  5. NEW Bing 现在共享 ChatGPT plus 插件；
  6. Media provenance tools，用于使用有关其来源的元数据标记和签名生成的内容的加密方法，使消费者能够验证图像或视频是否由 AI 生成。



Microsoft 与 OpenAI 的深度合作，涌现了 ChatGPT 这样的基础模型，以及在这些基础模型上构建的大量应用程序，这些都归功于 Microsoft 使用了端对端的平台来构建应用程序。

这些内容的基础是 Azure 强大计算平台、Windows 客户端 AI 开发平台。利用这些服务和基础模型构成了复杂的 Copilot 服务。

回到 GPT-4 这个基础模型，在过去的一段时间内，OpenAI 团队为 GPT-4 做了大量的调整，例如检查点和损失函数，并且重构了基础架构，带来了 “插件” 功能。但是相对的，GPT-4 仍然处于早期阶段，仍然具有很高的计算成本，并不是完全可用的。

而关于 Copilot 这项服务，其理念是打造一个多轮对话系统以帮助人们认知认识以外的事情，因此这不仅仅对于开发人员有用，现在有了开发 Copilot、生产力 Copilot、安全 Copilot，是一个为人人构建的 “副驾驶”。Microsoft 可以在短时间内推出了针对不同软件和服务的 Copilot，其原因是其构建了一个通用的技术堆栈，使得这项服务可以在短时间内迁移至其他任务。

<div align='center'><img src='../Picture/ce\屏幕截图 2023-05-24 153438.png' alt='' title='AI 模型训练开销对比'></div>

<br>基础模型使得人们在实现需求时无需重新构建非常复杂的东西，而只需专注于他们现在所需要做的应用程序需求。整个平台的基础模型，它们很强大，而且在持续变得更强，是可以被重用和推广的。然而基础模型并不能做所有事情，尤其是这些模型在预训练时未触及的领域，这时你可以使用各种各样的 “插件” 来适应你的工作需求，即使现有的模型并不足够完美。插件是一种取巧的做法，也可以增强你现有的 AI 系统。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 154748.png' alt='' title='插件是如何工作的'></div>

<br>应用程序的构建一般遵循这样的步骤：用户需求分析、应用程序结构、安全性和性能评估。在构建 Copilot 应用程序时，Microsoft 利用了一项名为 “语义内核” 的机制来编排业务流程，这项技术已经开源。

在业务流程的开始阶段，有很大一部分是提示（Prompt）和响应（response）筛选工作，这是由于它们可能会导致模型以不符合应用程序需求的方式响应或者做一些不安全的事情。

在某些情况下，需要元提示符，这是一组常设指令，这些指令被传递给模型。每次谈话都告诉模型如何适应你正在构建的 Copilot。例如，在 Bing 上，你可以设置响应是更加平衡还是更富有创造力。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 160151.png' alt='' title='构建 Copilot 应用程序的架构'></div>

<br>在设置元提示符和提示过滤阶段之后，就需要考虑检索增广生成，Grounding 就是向提示增加额外的上下文信息，这些上下文可能有助于帮助模型的响应。例如在 New Bing 中，用户使用普通 bing 向搜索索引查询相关文档，进行提示，Microsoft 就可以将这些文档添加到用于预训练模型的提示中，以便模型具有额外的上下文来提供良好的答案。

你可以使用插件在提示进入模型之前向提示添加一些额外的上下文信息。然后返回 Grounding 继续执行流程。

完成业务流程后，业务底层是基础架构和模型，可以基于 ChatGPT 或者 GPT-4，这些模型现已在 Azure Open AI API 服务上提供，你可以微调这些托管基础模型（GPT-4 将很快支持微调）。同时， Azure 也支持自定义模型，同时预置 GitHub 当中流行的开源模型。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 174446.png' alt='' title='Azure AI API 概览'></div>

<br>利用这些 API，Kevin Scott 提到了他的一个典型应用场景：利用 Whisper model 读取他的播客音频，然后利用 Dolly Model 获取播客中的访问嘉宾姓名，再利用 Bing searching Grounding API 生成关于嘉宾的简短介绍，利用 GPT-4 生成社交网络上介绍这一期播客内容的文章，再利用 DALL-E Model 生成符合文章内容的首图，最后再调用 Linkedin API 发布缩略图和文章内容，实现自动在社交媒体生成播客简要介绍的流程自动化。

## 有关 GPT 的最新信息

完整的 GPT 模型训练流程如下述所示：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 181536.png' alt='' title='完整的 GPT 模型训练流程'></div><br>

粗略来说，有四个主要的阶段：
  * 预训练
  * 监督微调
  * 奖励模型
  * 强化学习

在图中的每个阶段，都有对应的数据集以提供支持。OpenAI 设计了一套算法，这是训练神经网络的关键，也是目前性能领先的。接下来是结果模型，最后是一些注释。

在预训练阶段，图中展示的内容不是缩放的，因为这个阶段是所有计算工作发生的最基础的地方。此处会占据整个训练过程 99% 的计算时间，并且也会失败。这是处理互联网规模数据集的阶段，计算机中有数千个 GPU，并且可能还需要数月的训练。

其他三个阶段是微调阶段，只需要更少的 GPU 和时间。

### 预训练阶段

首先，需要收集大量数据。下面是一个称之为数据混合的示例，该示例来自 Meta 发布的论文。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 183251.png' alt='' title='数据集来源比例' height='300px'></div><br>

可以大致看到进入这些集合的数据集类型。首先是 Common Crawl，它只是网络抓取，C4 也是 Common Crawl。然后是一些高质量的数据集。例如，GitHub，维基百科，书籍，档案，Stack Exchange 等等。这些都混合在一起，然后根据一些给定的比例对它们进行采样，这形成了 GPT 神经网络的训练集。

在实际训练这些数据之前，需要再经历一个预处理步骤，那就是标记。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 184741.png' alt='' title='' height=''></div><br>

这是一个示例，展示把从互联网上抓取的原始文本的翻译成整数序列，这是 GPT 函数的原生表示。在这里展示了这些令牌的一些示例块，这是实际馈送到转换器的原始整数序列。

在这一阶段，文本片段、标记和整数之间的无损转换。这个阶段有许多算法，例如，可以使用字节配对编码之类的东西，它以迭代方式合并小文本块并将它们分组到标记中。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 185316.png' alt='' title='两个模型的超参数展示' height=''></div><br>

上图展示两个类似的例子来控制这个阶段的超参数。词汇量大致上是几个一万；上下文长度通常是 2048，现在甚至来到了 100,000。在语言模型中，输入文本的上下文长度决定了模型在生成下一个词或预测下一个词时所依赖的历史信息量。较长的上下文长度可以提供更多的上下文信息，有助于更准确地预测下一个词。然而，较长的上下文长度也会增加模型的计算复杂度和内存需求。

LLaMA 的参数数量大致是 650 亿。但是尽管 LLaMA 只有 65-B 的参数，而 GBT-3 的参数为 1750 亿个，LLaMA 是一个功能更强大的模型。这是因为该模型的训练时间要长得多，在这种情况下是 1.4 万亿个 tokens，而不仅仅是 3000 亿个 tokens。"tokens" 是指文本数据中的最小单位。在自然语言处理领域中，将文本划分为离散的单词、字符或子词等单元，称为 "tokens"。因此，不应该仅仅通过模型包含的参数数量来判断模型的性能。

进行预训练时，根据 tokens 布置数据阵列，并将其馈入 transformer。`B` 即为批大小（Batch size），指在深度学习模型中一次训练的样本数量；`T` 为最大上下文长度，图中是 10，也有可能是 2000 或者更大的数，这是一个非常长的行。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 192217.png' alt='' title='预训练阶段的基本过程' height=''></div><br>

上图中给出了 4 个文本的例子（4 个 batch，上下文长度 >= 10），在这个例子中，处理过程是：将所有的文字转换成 tokens；将这些数据阵列按顺序编排为一排；用特定的文本标记来分隔每段文本，就是告诉 transformer 新的文本从哪里开始；重新编排新的文本。

接下来先研究有颜色标记的单元格，假设中心点是绿的的单元格，那之前的所有 tokens 都将会被查看（也就是所有黄色的格），如果把这部分输入到 Tranformer 神经网络中，它将尝试预测序列中的下一个 tokens（红色单元格）。假设我们拥有的词汇量为 50257 个 tokens，则对于预测行为来说就有 50257 个数字，我们需要为接下来的预测指定一个概率分布。在现在，这个特定单元格中，出现的 tokens 是 513，那么我们就可以将其用作监督来源更新 transformer 模型的权重。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 153500.png' alt='' title='预训练阶段的基本过程' height=''></div><br>

将上述的工作应用于每个单元格中，就可以尝试使得 transformer 对序列中下一个 tokens 作出正确的预测。

下图展示了一个具体的 GPT 训练示例，来自莎士比亚的诗集：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 155949.png' alt='' title='利用训练数据采样训练后得到的 GPT 生成表现' height=''></div><br>

首先，GPT 从完全随机的权重开始，得到的结果就是完全随机的输出。但是随着训练 GPT 的时间越来越长，可以逐渐从模型和对训练文本的采样中获得越来越连贯和一致的文本。

另外一个有趣的事情是，假设有一个情绪分类的任务，以往的经验是收集一堆积极的和消极的东西，然后为此训练某种 LP 模型，但是现在的新方法是在数据集上先忽略情绪分类。在大规模语言模型上训练一个大规模的 Transformer，即使是只有几个示例，也可以非常有效地针对该任务微调模型。这是因为，Transfomer 被强制在语言建模任务上训练，仅仅在预测下一个 tokens 的任务上，它就已经理解了很多关于文本结构的信息，以及这些文本可能具备的概念。

这是在 GPT-1 上获得的经验。而在 GPT-2 的时候，人们注意到比微调更有效的方式是：给模型示例（Prompt）。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 193720.png' alt='使用 prompting（提示）比使用fine-tuning（微调）更加重要或者更受关注' title='基于示例-响应的微调的例子' height=''></div><br>

在上图中，有一些文章段落，然后用户问问题，并要求语言模型根据提示的信息和上下文做回答，当 Transformer 在尝试回答问题时，它实际上是在 “理解” 文档。将模型生成的响应（response）与期望的响应进行比较，可以通过计算得到生成的响应与期望响应之间的损失，根据损失信息进行反向传播，可以更新模型的权重参数。

可以看到即使没有额外地训练和微调任何神经网络，它在很多任务场景下都有很好的效果。这种思想开启了一个新的时代：利用示例-响应替代微调（fine-turing）。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 200222.png' alt='' title='基于 prompt-response 的基本模型进化树' height=''></div><br>

值得注意的是，GPT-4 并不是基本模型，而基本模型 GPT-3 可以通过达芬奇 API 调用，基本模型 GPT-2 在 GitHub 仓库中可以作为权重使用。但是目前最好的基本模型可能是 Meta 的 LLaMA 系列，尽管它没有商业许可。

基本模型不是 AI 助手，它只是在完成文本的生成，尽管有时候的 Question-Answer 会使得其像是一个助手，但准确性并不是特别可靠，在实践中效果并不是特别好，因此需要监督微调。

### 监督微调阶段

监督微调（SFT）阶段，将收集小而高质量的数据集。在这种情况下，我们将要求人类承包商从互联网上收集表格格式的、较新的和理想的响应数据，要收集大量此类数据，通常是数千个，这些数据应当是可以满足 QA 快速响应的数据。然后对这些数据进行语言建模。在算法层面上没有任何的改变，只是更换了数据集。

下图是一个理想的可用于监督微调的示例和响应：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 202652.png' alt='' title='' height=''></div><br>

### 奖励模型构建阶段

在这个阶段，数据集变成了模型对同一问题响应的比较。下图给出了在同一提示下，基于前述训练的 SFT 模型给出 3 个响应：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 204629.png' alt='' title='' height=''></div><br>

在这种情况下，需要人工对这 3 个响应进行排名。对于单个提示完成对，甚至可能需要花费数小时进行排名。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 205455.png' alt='' title='' height=''></div><br>

然后，将这些提示按行布局，在上图中所有三行的提示都是相同的，但完成情况各不相同。黄色的 tokens 来自 SFT 模型生成，绿色的 tokens 会有特殊的奖励值。只监督绿色 tokens 的 transformer，其将预测一些奖励值，以说明对于该提示的完成程度。这意味着它对每个响应的质量进行了猜测，再将人工排名的结果进行比对，其中有些结果可能相差较大，这表述为损失函数，然后再进行训练使得模型预测结果与人工排名一致，这就是训练奖励模型的方法，这使得我们能够对提示的完成程度进行评估。

### 强化学习阶段

一旦有了奖励模型，可以对任意给定提示的任意完成质量进行评分。在强化学习（RL）阶段，通过再次获取大量的提示，针对奖励模型进行强化学习。也即：将单个提示排成行；将模型进行 SFT；使用 SFT 模型初始化创建黄色的 tokens，然后再加入奖励 tokens，循环进行。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 212529.png' alt='' title='' height=''></div><br>

根据保持固定的奖励模型读取奖励，奖励模型会告诉我们这些提示的完成质量，因此可以构建损失函数。再上图的表格中，奖励模型认为第一行有相当高的完成质量，所以第一行采样的所有 tokens 都将获得增强，它们将来的预测都将会获得更高概率；而在第二行中，奖励模型不认为有较高的质量，因此在第二行采样的 tokens 在未来预测的概率都只会略微增强一点。

在很多提示的往复训练下，可以得到一个基本的策略：根据利用奖励模型的不断训练，由 SFT 模型创建的黄色 tokens 最终都会获得高分，这就是 RLHF（基于人类反馈的强化学习）。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 215522.png' alt='' title='imsys对目前的模型排名结果' height=''></div><br>

在上图给出的排名中，前三个都是 RLHF 模型，其余模型都是 SFT 模型。

### 对于 GPT 的开发路径

比较人类和 LLM 之间思考的差异会发现：对于写一篇文章，人类需要去查询数据，然后构思文章结构，也许还需要进行一些计算，在创作时可能会删删减减；但是对于 LLM 来说，创作只是不断地去创造下一个 tokens，去根据概率创作下一个词，它不能思考逻辑关系，然后删删减减文章内容。

基于此，对于 LLM 来说，需要其运行良好的条件可能是：首先给出几个提示和响应，transformer 会模仿那个模板，从而在该任务上做得更好。此外，你可以通过逐步的思考，引出 Transformer 比较理想的响应，就像是循循善诱。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 221230.png' alt='' title='imsys对目前的模型排名结果' height=''></div><br>




## 相关文章

  1. 如需获取 Kevin Scott 关于他实现流程自动化的代码，请点击[此处](github.com/microsoft/PodcastCopilot)。
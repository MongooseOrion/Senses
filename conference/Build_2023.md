# 微软开发者大会：Build 2023 会议记录

  * 时间：中国标准时间 2023 年 5 月 24 日 - 2023 年 5 月 25 日
  * 关键词：AI、Copilot、GPT、ARM

## 主要议题

  * Copilot 改变所有人的生活方式
  * 有关 GPT 的最新信息
  * 负责任地构建和使用 AI 模型
  * 畅想有 AI 参与的多个领域的未来
  * Windows ARM 取得新进展

## 属于 Copilot AI 的新纪元来临

### 宣布更新内容

  1. Microsoft 将 Copilot 服务扩展到多个应用和服务中，包括 Microsoft 365、GitHub Copilot、Windows Copilot，同时Microsoft 希望将 Copilot 作为一项基本服务，扩展为人人的 Copilot，并可供开发用户使用；
  2. Windows Copilot 发布，它将固定在任务栏，支持多模态的输入，为用户提供操作建议，并生成本地工作流和日常程序；
  3. 利用 Microsoft 365 Copilot 你可以通过简单的问询，在 Word 中生成书面文本，这一切都得益于 Bing 的联网搜索能力；
  4. Bing 现在加入 ChatGPT plus 插件，这意味着 ChatGPT 也将具备联网搜索能力，同时也将来到免费版；
  5. NEW Bing 现在共享 ChatGPT plus 插件；
  6. Media provenance tools，用于使用有关其来源的元数据标记和签名生成的内容的加密方法，使消费者能够验证图像或视频是否由 AI 生成；
  7. GitHub Copilot X，使用 GPT-4 驱动，支持功能包括：Copilot chat, Copilot pull request 和自动生成代码。其中copilot chat 可以直接根据用户指令读取和优化代码。

### Copilot 的一些进展

Microsoft 与 OpenAI 的深度合作，涌现了 ChatGPT 这样的基础模型，以及在这些基础模型上构建的大量应用程序，这些都归功于 Microsoft 使用了端对端的平台来构建应用程序。

这些内容的基础是 Azure 强大计算平台、Windows 客户端 AI 开发平台。利用这些服务和基础模型构成了复杂的 Copilot 服务。

回到 GPT-4 这个基础模型，在过去的一段时间内，OpenAI 团队为 GPT-4 做了大量的调整，例如检查点和损失函数，并且重构了基础架构，带来了 “插件” 功能。但是相对的，GPT-4 仍然处于早期阶段，仍然具有很高的计算成本，并不是完全可用的。

而关于 Copilot 这项服务，其理念是打造一个多轮对话系统以帮助人们认知认识以外的事情，因此这不仅仅对于开发人员有用，现在有了开发 Copilot、生产力 Copilot、安全 Copilot，是一个为人人构建的 “副驾驶”。Microsoft 可以在短时间内推出了针对不同软件和服务的 Copilot，其原因是其构建了一个通用的技术堆栈，使得这项服务可以在短时间内迁移至其他任务。

<div align='center'><img src='../Picture/ce\屏幕截图 2023-05-24 153438.png' alt='' title='AI 模型训练开销对比'></div>

<br>基础模型使得人们在实现需求时无需重新构建非常复杂的东西，而只需专注于他们现在所需要做的应用程序需求。整个平台的基础模型，它们很强大，而且在持续变得更强，是可以被重用和推广的。然而基础模型并不能做所有事情，尤其是这些模型在预训练时未触及的领域，这时你可以使用各种各样的 “插件” 来适应你的工作需求，即使现有的模型并不足够完美。插件是一种取巧的做法，也可以增强你现有的 AI 系统。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 154748.png' alt='' title='插件是如何工作的'></div>

<br>应用程序的构建一般遵循这样的步骤：用户需求分析、应用程序结构、安全性和性能评估。在构建 Copilot 应用程序时，Microsoft 利用了一项名为 “语义内核” 的机制来编排业务流程，这项技术已经开源。

在业务流程的开始阶段，有很大一部分是提示（Prompt）和响应（response）筛选工作，这是由于它们可能会导致模型以不符合应用程序需求的方式响应或者做一些不安全的事情。

在某些情况下，需要元提示符，这是一组常设指令，这些指令被传递给模型。每次谈话都告诉模型如何适应你正在构建的 Copilot。例如，在 Bing 上，你可以设置响应是更加平衡还是更富有创造力。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 160151.png' alt='' title='构建 Copilot 应用程序的架构'></div>

<br>在设置元提示符和提示过滤阶段之后，就需要考虑检索增广生成，Grounding 就是向提示增加额外的上下文信息，这些上下文可能有助于帮助模型的响应。例如在 New Bing 中，用户使用普通 bing 向搜索索引查询相关文档，进行提示，Microsoft 就可以将这些文档添加到用于预训练模型的提示中，以便模型具有额外的上下文来提供良好的答案。

你可以使用插件在提示进入模型之前向提示添加一些额外的上下文信息。然后返回 Grounding 继续执行流程。

完成业务流程后，业务底层是基础架构和模型，可以基于 ChatGPT 或者 GPT-4，这些模型现已在 Azure Open AI API 服务上提供，你可以微调这些托管基础模型（GPT-4 将很快支持微调）。同时， Azure 也支持自定义模型，同时预置 GitHub 当中流行的开源模型。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 174446.png' alt='' title='Azure AI API 概览'></div>

<br>利用这些 API，Kevin Scott 提到了他的一个典型应用场景：利用 Whisper model 读取他的播客音频，然后利用 Dolly Model 获取播客中的访问嘉宾姓名，再利用 Bing searching Grounding API 生成关于嘉宾的简短介绍，利用 GPT-4 生成社交网络上介绍这一期播客内容的文章，再利用 DALL-E Model 生成符合文章内容的首图，最后再调用 Linkedin API 发布缩略图和文章内容，实现自动在社交媒体生成播客简要介绍的流程自动化。

### 使用 Azure 开发下一代 AI

ChatGPT Plus 在过去的一段时间内先后发布了多款插件应用，获得了各种能力。现在，由开发者自己构建的 ChatGPT 插件可以直接在 chatgpt 插件商店里自定义部署（只为自己使用）。例如：首先在 github codespace 运行代码，打开 API 端口，复制 URL，粘贴到 chatgpt 的自定义插件位置，就可以实现自定义插件的安装。同时，也可以利用这种方式实现对自己 API 的调试。

全新的 Azure AI studio 允许开发者利用 Azure OpenAI api，直接从 PDF 和 word 文档中获取你的内容，然后定制化你自己的 GPT，这样可以增强 GPT 在特定任务中的回答能力，例如消费者咨询退货政策。Azure AI studio 目前包含数千个 AI 模型，可利用 Azure AI 模型目录非常轻松地使用每个开源模型。

Azure AI Content Safety，可用于测试和评估 AI 部署的安全性。自动检测不良内容，从而更轻松地确保在线社区、应用程序和 AI 系统的安全。最基本的，安全性被直接构建在模型中，以便它识别到有问题的输出时做出适当的响应。

而另外两层是：安全系统和元提示符层，这部分由开发者自定义。


## 有关 GPT 的最新信息

完整的 GPT 模型训练流程如下述所示：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 181536.png' alt='' title='完整的 GPT 模型训练流程'></div><br>

粗略来说，有四个主要的阶段：
  * 预训练
  * 监督微调
  * 奖励模型
  * 强化学习

在图中的每个阶段，都有对应的数据集以提供支持。OpenAI 设计了一套算法，这是训练神经网络的关键，也是目前性能领先的。接下来是结果模型，最后是一些注释。

在预训练阶段，图中展示的内容不是缩放的，因为这个阶段是所有计算工作发生的最基础的地方。此处会占据整个训练过程 99% 的计算时间，并且也会失败。这是处理互联网规模数据集的阶段，计算机中有数千个 GPU，并且可能还需要数月的训练。

其他三个阶段是微调阶段，只需要更少的 GPU 和时间。

### 预训练阶段

首先，需要收集大量数据。下面是一个称之为数据混合的示例，该示例来自 Meta 发布的论文。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 183251.png' alt='' title='数据集来源比例' height='300px'></div><br>

可以大致看到进入这些集合的数据集类型。首先是 Common Crawl，它只是网络抓取，C4 也是 Common Crawl。然后是一些高质量的数据集。例如，GitHub，维基百科，书籍，档案，Stack Exchange 等等。这些都混合在一起，然后根据一些给定的比例对它们进行采样，这形成了 GPT 神经网络的训练集。

在实际训练这些数据之前，需要再经历一个预处理步骤，那就是标记。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 184741.png' alt='' title='' height=''></div><br>

这是一个示例，展示把从互联网上抓取的原始文本的翻译成整数序列，这是 GPT 函数的原生表示。在这里展示了这些令牌的一些示例块，这是实际馈送到转换器的原始整数序列。

在这一阶段，文本片段、标记和整数之间的无损转换。这个阶段有许多算法，例如，可以使用字节配对编码之类的东西，它以迭代方式合并小文本块并将它们分组到标记中。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 185316.png' alt='' title='两个模型的超参数展示' height=''></div><br>

上图展示两个类似的例子来控制这个阶段的超参数。词汇量大致上是几个一万；上下文长度通常是 2048，现在甚至来到了 100,000。在语言模型中，输入文本的上下文长度决定了模型在生成下一个词或预测下一个词时所依赖的历史信息量。较长的上下文长度可以提供更多的上下文信息，有助于更准确地预测下一个词。然而，较长的上下文长度也会增加模型的计算复杂度和内存需求。

LLaMA 的参数数量大致是 650 亿。但是尽管 LLaMA 只有 65-B 的参数，而 GBT-3 的参数为 1750 亿个，LLaMA 是一个功能更强大的模型。这是因为该模型的训练时间要长得多，在这种情况下是 1.4 万亿个 tokens，而不仅仅是 3000 亿个 tokens。"tokens" 是指文本数据中的最小单位。在自然语言处理领域中，将文本划分为离散的单词、字符或子词等单元，称为 "tokens"。因此，不应该仅仅通过模型包含的参数数量来判断模型的性能。

进行预训练时，根据 tokens 布置数据阵列，并将其馈入 transformer。`B` 即为批大小（Batch size），指在深度学习模型中一次训练的样本数量；`T` 为最大上下文长度，图中是 10，也有可能是 2000 或者更大的数，这是一个非常长的行。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 192217.png' alt='' title='预训练阶段的基本过程' height=''></div><br>

上图中给出了 4 个文本的例子（4 个 batch，上下文长度 >= 10），在这个例子中，处理过程是：将所有的文字转换成 tokens；将这些数据阵列按顺序编排为一排；用特定的文本标记来分隔每段文本，就是告诉 transformer 新的文本从哪里开始；重新编排新的文本。

接下来先研究有颜色标记的单元格，假设中心点是绿的的单元格，那之前的所有 tokens 都将会被查看（也就是所有黄色的格），如果把这部分输入到 Tranformer 神经网络中，它将尝试预测序列中的下一个 tokens（红色单元格）。假设我们拥有的词汇量为 50257 个 tokens，则对于预测行为来说就有 50257 个数字，我们需要为接下来的预测指定一个概率分布。在现在，这个特定单元格中，出现的 tokens 是 513，那么我们就可以将其用作监督来源更新 transformer 模型的权重。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 153500.png' alt='' title='预训练阶段的基本过程' height=''></div><br>

将上述的工作应用于每个单元格中，就可以尝试使得 transformer 对序列中下一个 tokens 作出正确的预测。

下图展示了一个具体的 GPT 训练示例，来自莎士比亚的诗集：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 155949.png' alt='' title='利用训练数据采样训练后得到的 GPT 生成表现' height=''></div><br>

首先，GPT 从完全随机的权重开始，得到的结果就是完全随机的输出。但是随着训练 GPT 的时间越来越长，可以逐渐从模型和对训练文本的采样中获得越来越连贯和一致的文本。

另外一个有趣的事情是，假设有一个情绪分类的任务，以往的经验是收集一堆积极的和消极的东西，然后为此训练某种 LP 模型，但是现在的新方法是在数据集上先忽略情绪分类。在大规模语言模型上训练一个大规模的 Transformer，即使是只有几个示例，也可以非常有效地针对该任务微调模型。这是因为，Transfomer 被强制在语言建模任务上训练，仅仅在预测下一个 tokens 的任务上，它就已经理解了很多关于文本结构的信息，以及这些文本可能具备的概念。

这是在 GPT-1 上获得的经验。而在 GPT-2 的时候，人们注意到比微调更有效的方式是：给模型示例（Prompt）。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 193720.png' alt='使用 prompting（提示）比使用fine-tuning（微调）更加重要或者更受关注' title='基于示例-响应的微调的例子' height=''></div><br>

在上图中，有一些文章段落，然后用户问问题，并要求语言模型根据提示的信息和上下文做回答，当 Transformer 在尝试回答问题时，它实际上是在 “理解” 文档。将模型生成的响应（response）与期望的响应进行比较，可以通过计算得到生成的响应与期望响应之间的损失，根据损失信息进行反向传播，可以更新模型的权重参数。

可以看到即使没有额外地训练和微调任何神经网络，它在很多任务场景下都有很好的效果。这种思想开启了一个新的时代：利用示例-响应替代微调（fine-turing）。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 200222.png' alt='' title='基于 prompt-response 的基本模型进化树' height=''></div><br>

值得注意的是，GPT-4 并不是基本模型，而基本模型 GPT-3 可以通过达芬奇 API 调用，基本模型 GPT-2 在 GitHub 仓库中可以作为权重使用。但是目前最好的基本模型可能是 Meta 的 LLaMA 系列，尽管它没有商业许可。

基本模型不是 AI 助手，它只是在完成文本的生成，尽管有时候的 Question-Answer 会使得其像是一个助手，但准确性并不是特别可靠，在实践中效果并不是特别好，因此需要监督微调。

### 监督微调阶段

监督微调（SFT）阶段，将收集小而高质量的数据集。在这种情况下，我们将要求人类承包商从互联网上收集表格格式的、较新的和理想的响应数据，要收集大量此类数据，通常是数千个，这些数据应当是可以满足 QA 快速响应的数据。然后对这些数据进行语言建模。在算法层面上没有任何的改变，只是更换了数据集。

下图是一个理想的可用于监督微调的示例和响应：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 202652.png' alt='' title='' height=''></div><br>

### 奖励模型构建阶段

在这个阶段，数据集变成了模型对同一问题响应的比较。下图给出了在同一提示下，基于前述训练的 SFT 模型给出 3 个响应：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 204629.png' alt='' title='' height=''></div><br>

在这种情况下，需要人工对这 3 个响应进行排名。对于单个提示完成对，甚至可能需要花费数小时进行排名。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 205455.png' alt='' title='' height=''></div><br>

然后，将这些提示按行布局，在上图中所有三行的提示都是相同的，但完成情况各不相同。黄色的 tokens 来自 SFT 模型生成，绿色的 tokens 会有特殊的奖励值。只监督绿色 tokens 的 transformer，其将预测一些奖励值，以说明对于该提示的完成程度。这意味着它对每个响应的质量进行了猜测，再将人工排名的结果进行比对，其中有些结果可能相差较大，这表述为损失函数，然后再进行训练使得模型预测结果与人工排名一致，这就是训练奖励模型的方法，这使得我们能够对提示的完成程度进行评估。

### 强化学习阶段

一旦有了奖励模型，可以对任意给定提示的任意完成质量进行评分。在强化学习（RL）阶段，通过再次获取大量的提示，针对奖励模型进行强化学习。也即：将单个提示排成行；将模型进行 SFT；使用 SFT 模型初始化创建黄色的 tokens，然后再加入奖励 tokens，循环进行。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 212529.png' alt='' title='' height=''></div><br>

根据保持固定的奖励模型读取奖励，奖励模型会告诉我们这些提示的完成质量，因此可以构建损失函数。再上图的表格中，奖励模型认为第一行有相当高的完成质量，所以第一行采样的所有 tokens 都将获得增强，它们将来的预测都将会获得更高概率；而在第二行中，奖励模型不认为有较高的质量，因此在第二行采样的 tokens 在未来预测的概率都只会略微增强一点。

在很多提示的往复训练下，可以得到一个基本的策略：根据利用奖励模型的不断训练，由 SFT 模型创建的黄色 tokens 最终都会获得高分，这就是 RLHF（基于人类反馈的强化学习）。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 215522.png' alt='' title='imsys对目前的模型排名结果' height=''></div><br>

在上图给出的排名中，前三个都是 RLHF 模型，其余模型都是 SFT 模型。

### 对于 GPT 的开发路径思考

比较人类和 LLM 之间思考的差异会发现：对于写一篇文章，人类需要去查询数据，然后构思文章结构，也许还需要进行一些计算，在创作时可能会删删减减；但是对于 LLM 来说，创作只是不断地去创造下一个 tokens，去根据概率创作下一个词，它不能思考逻辑关系，不能纠正错误，然后删删减减文章内容。

基于此，对于 LLM 来说，需要其运行良好的条件可能是：首先给出几个提示和响应，transformer 会模仿那个模板或预设条件，从而在该任务上做得更好。此外，你可以通过逐步的思考，引出 Transformer 比较理想的响应，就像是循循善诱。例如给出 prompt：假设你的 IQ 有 120、假设你是这个领域的专家...

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-25 221230.png' alt='' title='imsys对目前的模型排名结果' height=''></div><br>

另外一个现象是，LLMs 实际上经常能发现对于某个任务并没有很好地完成。例如在下图中，要求 GPT-4 生成一首不押韵的诗，但实际上它生成的是押韵的诗，但如果你继续问它你完成任务了吗？它是很清楚并没有完成任务的。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 130220.png' alt='' title='' height=''></div><br>

但是另一方面，如果没有你去提醒它，它就不会注意到这一点，因此你必须在提示中告诉它，让它检查。

### 对于 LLMs 的新的开发或调试方式

在默认情况下，LLMs 并不知道例如计算器、代码生成器这样的角色，因此需要引入插件。

在之前，例如 GPT-3，只能读取它自身的记忆，然而在例如 Bing 的检索工具上存储着大量的信息。对于 Transformer 而言，它的上下文窗口就是工作内存，假设将与任务有关的所有数据加载到工作内存，这是可行的，且模型将更加地有效。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 131331.png' alt='' title='' height=''></div><br>

通过将这些插件接入大模型当中，你可以索引它们所有的数据。在开发时，你可以获取相关文档，分割成块，用向量标识，在测试时对这些向量进行某种查询，获得与任务相关的块，并将其填充到 prompt 中。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 132627.png' alt='' title='微调的最新做法和流程' height=''></div><br>

下图是一些优化模型的建议，其中第一部分是实现最佳性能，第二部分是按顺序优化性能。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 132955.png' alt='' title='' height=''></div><br>

下图是一些 LLMs 使用可能出现的情况：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 133710.png' alt='' title='' height=''></div><br>


## 利用 AI 为开发人员助力未来

### 负责任地构建和使用 AI 模型

构建 AI 模型的首要准则是：确保 AI 系统公平、包容、可靠、安全（safety）、安全（security）和透明。开发者需要对他们的开发和使用方式负责。

如何控制人工智能的社会影响以及减轻使用 AI 技术而导致的后果非常重要，我们应该认识到，构建更加负责任的 AI 的技术手段正在不断发展，这其中即有创新，也有我们从错误中吸取的教训。随着 AI 在商业领域和公共部门的使用不断扩大，我们必须继续促进企业、政府、非政府组织（NGO）、学术研究人员以及所有其他感兴趣的个人和组织之间的公开对话，以促进负责任地使用 AI，以及为社会做好应对其影响的准备。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-29 190330.png' alt='' title='构建负责任的 AI 时的一些目标或者衡量标准' height=''></div><br>

在构建 AI 时，首先是需要确定可能存在的危害是什么，以及面临的风险是什么。例如在开放 GPT-4 时，微软召集了各类的专家，研究这项技术的核心挑战是什么，然后在模型或者平台中构建保护措施。

其次是需要测试用户可以让系统工作在正常设定之外（越狱）的可能性。测试是构建任何生成式 AI 的重要步骤，它使得抽象的 “风险概念” 得以以直观的概率出现在开发者面前。

等你了解了风险情况，你就应该着手改进这些可能的越狱漏洞。

构建安全系统时，分为以下几个层级：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-29 193004.png' alt='' title='混合层' height=''></div><br>

第一层，在模型上，采用基于人类反馈的强化学习（RLHF）技术，训练模型在查看敏感或者有害的信息时做出适当的反应。

第二层是安全系统，当模型出错时，安全系统可以捕获它们，Azure 内容安全系统使你能够检测、识别有害内容，并实时采取行动。该系统拥有更强的上下文理解能力，而不仅仅是在关键字上工作。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-29 194114.png' alt='' title='混合层' height=''></div><br>

其支持图像和文字的过滤，涵盖 4 类内容，而音频和视频支持也即将到来。

### 元宇宙与 AI



## Windows on ARM 的最新信息

### 利用 ARM 本地运行 AI

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 172219.png' alt='' title='骁龙 AI 堆栈结构' height=''></div><br>

骁龙处理器广泛应用于移动设备中，这意味着若要使用骁龙处理器部署 AI，你可以在云、边缘甚至是汽车中使用相同的堆栈运行任何兼容的内容。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-26 175419.png' alt='' title='在 Windows 上利用骁龙处理器部署生成式 AI' height=''></div><br>

### WSA 的最新消息

在过去的一年里，WSA 进行了大量改进，以提高运行时启动性能、图形以及深度 Windows 集成。现在，Microsoft 已经在 WSA 上提供了对Android 13 的支持，以及画中画和对应用程序链接的支持。

同时，WSA 也在持续改进以支持文件共享、本地网络访问，还有更好地与 Android Studio 和 Visual Studio 集成。

同时，Amazon 即将发布一个针对键盘、触控笔和鼠标的无缝映射 API，以将这些操作映射到游戏中的操作。

## 相关文章

  1. 如需获取 Kevin Scott 关于他实现流程自动化的代码，请点击[此处](github.com/microsoft/PodcastCopilot)。
  2. 如需获取 Azure OpenAI 插件示例开发代码，请点击[此处](https://aka.ms/PluginRepo)。
  3. 有关构建负责任的 AI 模型的更多内容，请[点此](https://learn.microsoft.com/en-us/training/modules/responsible-ai-principles/1-introduction)访问。
  4. 对于 WSA 的反馈渠道，请[点此](https://github.com/microsoft/WSA)了解。
# 微软开发者大会：Build 2023 会议记录

  * 时间：中国标准时间 2023 年 5 月 24 日 - 2023 年 5 月 25 日
  * 关键词：AI、Copilot、GPT

## 主要议题

  * Copilot 改变所有人的生活方式
  * GPT 的现状



## 属于 Copilot AI 的新纪元来临

### 宣布更新内容

  1. Microsoft 将 Copilot 服务扩展到多个应用和服务中，包括 Microsoft 365、GitHub Copilot、Windows Copilot，同时Microsoft 希望将 Copilot 作为一项基本服务，扩展为人人的 Copilot，并可供开发用户使用；
  2. Windows Copilot 发布，它将固定在任务栏，支持多模态的输入，为用户提供操作建议，并生成本地工作流和日常程序；
  3. 利用 Microsoft 365 Copilot 你可以通过简单的问询，在 Word 中生成书面文本，这一切都得益于 Bing 的联网搜索能力；
  4. Bing 现在加入 ChatGPT plus 插件，这意味着 ChatGPT 也将具备联网搜索能力，同时也将来到免费版；
  5. NEW Bing 现在共享 ChatGPT plus 插件；
  6. Media provenance tools，用于使用有关其来源的元数据标记和签名生成的内容的加密方法，使消费者能够验证图像或视频是否由 AI 生成。



Microsoft 与 OpenAI 的深度合作，涌现了 ChatGPT 这样的基础模型，以及在这些基础模型上构建的大量应用程序，这些都归功于 Microsoft 使用了端对端的平台来构建应用程序。

这些内容的基础是 Azure 强大计算平台、Windows 客户端 AI 开发平台。利用这些服务和基础模型构成了复杂的 Copilot 服务。

回到 GPT-4 这个基础模型，在过去的一段时间内，OpenAI 团队为 GPT-4 做了大量的调整，例如检查点和损失函数，并且重构了基础架构，带来了 “插件” 功能。但是相对的，GPT-4 仍然处于早期阶段，仍然具有很高的计算成本，并不是完全可用的。

而关于 Copilot 这项服务，其理念是打造一个多轮对话系统以帮助人们认知认识以外的事情，因此这不仅仅对于开发人员有用，现在有了开发 Copilot、生产力 Copilot、安全 Copilot，是一个为人人构建的 “副驾驶”。Microsoft 可以在短时间内推出了针对不同软件和服务的 Copilot，其原因是其构建了一个通用的技术堆栈，使得这项服务可以在短时间内迁移至其他任务。

<div align='center'><img src='../Picture/ce\屏幕截图 2023-05-24 153438.png' alt='' title='AI 模型训练开销对比'></div>

<br>基础模型使得人们在实现需求时无需重新构建非常复杂的东西，而只需专注于他们现在所需要做的应用程序需求。整个平台的基础模型，它们很强大，而且在持续变得更强，是可以被重用和推广的。然而基础模型并不能做所有事情，尤其是这些模型在预训练时未触及的领域，这时你可以使用各种各样的 “插件” 来适应你的工作需求，即使现有的模型并不足够完美。插件是一种取巧的做法，也可以增强你现有的 AI 系统。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 154748.png' alt='' title='插件是如何工作的'></div>

<br>应用程序的构建一般遵循这样的步骤：用户需求分析、应用程序结构、安全性和性能评估。在构建 Copilot 应用程序时，Microsoft 利用了一项名为 “语义内核” 的机制来编排业务流程，这项技术已经开源。

在业务流程的开始阶段，有很大一部分是提示（Prompt）和响应（response）筛选工作，这是由于它们可能会导致模型以不符合应用程序需求的方式响应或者做一些不安全的事情。

在某些情况下，需要元提示符，这是一组常设指令，这些指令被传递给模型。每次谈话都告诉模型如何适应你正在构建的 Copilot。例如，在 Bing 上，你可以设置响应是更加平衡还是更富有创造力。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 160151.png' alt='' title='构建 Copilot 应用程序的架构'></div>

<br>在设置元提示符和提示过滤阶段之后，就需要考虑检索增广生成，Grounding 就是向提示增加额外的上下文信息，这些上下文可能有助于帮助模型的响应。例如在 New Bing 中，用户使用普通 bing 向搜索索引查询相关文档，进行提示，Microsoft 就可以将这些文档添加到用于预训练模型的提示中，以便模型具有额外的上下文来提供良好的答案。

你可以使用插件在提示进入模型之前向提示添加一些额外的上下文信息。然后返回 Grounding 继续执行流程。

完成业务流程后，业务底层是基础架构和模型，可以基于 ChatGPT 或者 GPT-4，这些模型现已在 Azure Open AI API 服务上提供，你可以微调这些托管基础模型（GPT-4 将很快支持微调）。同时， Azure 也支持自定义模型，同时预置 GitHub 当中流行的开源模型。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 174446.png' alt='' title='Azure AI API 概览'></div>

<br>利用这些 API，Kevin Scott 提到了他的一个典型应用场景：利用 Whisper model 读取他的播客音频，然后利用 Dolly Model 获取播客中的访问嘉宾姓名，再利用 Bing searching Grounding API 生成关于嘉宾的简短介绍，利用 GPT-4 生成社交网络上介绍这一期播客内容的文章，再利用 DALL-E Model 生成符合文章内容的首图，最后再调用 Linkedin API 发布缩略图和文章内容，实现自动在社交媒体生成播客简要介绍的流程自动化。

## 有关 GPT 的最新信息

完整的 GPT 模型训练流程如下述所示：

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 181536.png' alt='' title='完整的 GPT 模型训练流程'></div><br>

粗略来说，有四个主要的阶段：
  * 预训练
  * 监督微调
  * 奖励模型
  * 强化学习

在图中的每个阶段，都有对应的数据集以提供支持。OpenAI 设计了一套算法，这是训练神经网络的关键，也是目前性能领先的。接下来是结果模型，最后是一些注释。

在预训练阶段，图中展示的内容不是缩放的，因为这个阶段是所有计算工作发生的最基础的地方。此处会占据整个训练过程 99% 的计算时间，并且也会失败。这是处理互联网规模数据集的阶段，计算机中有数千个 GPU，并且可能还需要数月的训练。

其他三个阶段是微调阶段，只需要更少的 GPU 和时间。

### 预训练阶段

首先，需要收集大量数据。下面是一个称之为数据混合的示例，该示例来自 Meta 发布的论文。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 183251.png' alt='' title='数据集来源比例' height='300px'></div><br>

可以大致看到进入这些集合的数据集类型。首先是 Common Crawl，它只是网络抓取，C4 也是 Common Crawl。然后是一些高质量的数据集。例如，GitHub，维基百科，书籍，档案，Stack Exchange 等等。这些都混合在一起，然后根据一些给定的比例对它们进行采样，这形成了 GPT 神经网络的训练集。

在实际训练这些数据之前，需要再经历一个预处理步骤，那就是标记。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 184741.png' alt='' title='' height=''></div><br>

这是一个示例，展示把从互联网上抓取的原始文本的翻译成整数序列，这是 GPT 函数的原生表示。在这里展示了这些令牌的一些示例块，这是实际馈送到转换器的原始整数序列。

在这一阶段，文本片段、标记和整数之间的无损转换。这个阶段有许多算法，例如，可以使用字节配对编码之类的东西，它以迭代方式合并小文本块并将它们分组到标记中。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 185316.png' alt='' title='' height=''></div><br>

上图展示两个类似的例子来控制这个阶段的超参数。词汇量大致上是几个一万；上下文长度通常是 2048，现在甚至来到了 100,000，这控制了 GPT 在尝试预测序列中的下一个整数时将查看的最大整数数。LLaMA 的参数数量大致是 650 亿。但是尽管 LLaMA 只有 65-B 的参数，而 GBT-3 的参数为 1750 亿个，LLaMA 是一个功能更强大的模型。这是因为该模型的训练时间要长得多，在这种情况下是 1.4 万亿个token，而不仅仅是 3000 亿个 token。因此，不应该仅仅通过模型包含的参数数量来判断模型的性能。

<div align='center'><img src='../Picture\ce\屏幕截图 2023-05-24 192217.png' alt='' title='' height=''></div><br>



## 相关文章

  1. 如需获取 Kevin Scott 关于他实现流程自动化的代码，请点击[此处](github.com/microsoft/PodcastCopilot)。